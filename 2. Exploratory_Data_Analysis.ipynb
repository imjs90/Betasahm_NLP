{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the document-term matrix\n",
    "import pandas as pd\n",
    "\n",
    "data = pd.read_pickle('dtm.pkl')\n",
    "data = data.transpose()\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the top 30 words each companies\n",
    "top_dict = {}\n",
    "\n",
    "for c in data.columns:\n",
    "    top = data[c].sort_values(ascending = False).head(30)\n",
    "    top_dict[c] = list(zip(top.index, top.values))\n",
    "\n",
    "top_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the top 15 words said by each companies\n",
    "for companies, top_words in top_dict.items():\n",
    "    print(companies)\n",
    "    print(','.join([word for word, count in top_words[0:14]]))\n",
    "    print('---')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at the most common top words --> add them to the stop word list\n",
    "from collections import Counter\n",
    "\n",
    "# Let's first pull out the top 30 words for each companies\n",
    "words = []\n",
    "for companies in data.columns:\n",
    "    top = [word for (word, count) in top_dict[companies]]\n",
    "    for t in top:\n",
    "        words.append(t)\n",
    "        \n",
    "words\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's aggregate this list and identify the most common words\n",
    "Counter(words).most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If more than half of the companies have it as a top word, exclude it from the list\n",
    "add_stop_words = [word for word, count in Counter(words).most_common() if count > 500]\n",
    "add_stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = ['را', 'با', 'است', 'می', 'که', 'این', 'از', 'به', 'در', 'و', 'های', 'برای', 'آن', 'یک', 'ها',\n",
    "             'شود', 'شده', 'خود', 'کرد', 'ای', 'کرده', 'داشته', 'بوده', 'بود', 'زده', 'تا', 'هر', 'هم', 'نیز',\n",
    "             'خواهد', 'شد', 'بر', 'دارد', 'زد', 'آن', 'یا', 'باشد', 'میگردد', 'ای' ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's update our document-term matrix with the new list of stop words\n",
    "\n",
    "from sklearn.feature_extraction import text \n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Read in cleaned data\n",
    "data_clean = pd.read_pickle('data_clean.pkl')\n",
    "\n",
    "# Add new stop words\n",
    "stop_words = add_stop_words + stop_words\n",
    "\n",
    "# Recreate document-term matrix\n",
    "cv = CountVectorizer(stop_words=stop_words)\n",
    "data_cv = cv.fit_transform(data_clean.Post)\n",
    "data_stop = pd.DataFrame(data_cv.toarray(), columns=cv.get_feature_names())\n",
    "data_stop.index = data_clean.hashtags\n",
    "\n",
    "# Pickle it for later use\n",
    "import pickle\n",
    "pickle.dump(cv, open(\"cv_stop.pkl\", \"wb\"))\n",
    "data_stop.to_pickle(\"dtm_stop.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the number of unique words that each companies uses\n",
    "\n",
    "unique_list = []\n",
    "for companies in data.columns:\n",
    "    uniques = data[companies].nonzero()[0].size\n",
    "    unique_list.append(uniques)\n",
    "data_words = pd.DataFrame(list(zip(data_clean.hashtags, unique_list)), columns=['companies',\"unique_words\"])\n",
    "data_unique_sort = data_words.sort_values(by= \"unique_words\")\n",
    "data_unique_sort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find total number of words which a companies uses\n",
    "\n",
    "total_list = []\n",
    "\n",
    "for companies in data.columns:\n",
    "    totals = sum(data[companies])\n",
    "    total_list.append(totals)\n",
    "\n",
    "data_words['total_words'] = total_list\n",
    "\n",
    "data_wpm_sort = data_words.sort_values(by ='total_words' )\n",
    "data_wpm_sort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_wpm_sort =data_wpm_sort[data_wpm_sort['unique_words'] != data_wpm_sort['total_words']]\n",
    "data_wpm_sort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corrmat = data_wpm_sort.drop(labels=['companies'], axis=1)\n",
    "corrmat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlated_data = corrmat.corr()\n",
    "correlated_data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(corrmat)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Counter(words).most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# number of negative words for every companies\n",
    "data_bad_words = data.transpose()[['زیان' ,'افت' ,'کاهش' ,'تعلیق' ,'منفی' ,'بدهی']]\n",
    "\n",
    "\n",
    "data_bw = pd.concat([data_bad_words.زیان + data_bad_words.افت +\n",
    "                     data_bad_words.کاهش + data_bad_words.تعلیق +\n",
    "                     data_bad_words.منفی + data_bad_words.بدهی]) \n",
    "\n",
    "data_bw = pd.DataFrame(data_bw, columns = ['bad_words'])\n",
    "data_bw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of positive words for every companies\n",
    "data_good_words = data.transpose()[['سود','افزایش','رشد','مثبت','پیشرفت','بهبود']]\n",
    "\n",
    "data_gw = pd.concat([data_good_words.پیشرفت + data_good_words.رشد +\n",
    "                     data_good_words.سود + data_good_words.افزایش +\n",
    "                     data_good_words.مثبت + data_good_words.بهبود])\n",
    "\n",
    "\n",
    "data_gw = pd.DataFrame(data_gw, columns = ['good_words'])\n",
    "data_gw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_gw['bad_words'] = data_bw.bad_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_gbw = data_gw.sort_values(by ='good_words', ascending = False)\n",
    "data_gbw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove rows with zero good and bad words\n",
    "data_gbw =data_gbw[data_gbw['good_words'] != data_gbw['bad_words']]\n",
    "data_gbw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_gbw.index.name = ''\n",
    "data_gbw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_gbw.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlated_data = data_gbw.corr()\n",
    "correlated_data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(data_gbw)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = [15, 10]\n",
    "\n",
    "for i, companies in enumerate(data_gbw.index):\n",
    "    x = data_gbw.good_words.loc[companies]\n",
    "    y = data_gbw.bad_words.loc[companies]\n",
    "    plt.scatter(x, y, color = \"blue\")\n",
    "    #plt.text(x+1.5, y+0.5, data_gbw.index[i], fontsize = 10 )\n",
    "    #plt.xlim(-5, 155)\n",
    "    \n",
    "plt.title('Number of Bad and Good Words')\n",
    "plt.xlabel('Number of Good Words', fontsize =15)\n",
    "plt.ylabel('Number of Bad Words', fontsize=15);\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = [15, 10]\n",
    "\n",
    "for i, companies in enumerate(data_gbw.index):\n",
    "    x = data_gbw.good_words.loc[companies]\n",
    "    y = data_gbw.bad_words.loc[companies]\n",
    "    plt.scatter(x, y, color = \"blue\")\n",
    "    plt.text(x+1.5, y+0.5, data_gbw.index[i], fontsize = 10 )\n",
    "    plt.xlim(-5, 155)\n",
    "    \n",
    "plt.title('Number of Bad and Good Words')\n",
    "plt.xlabel('Number of Good Words', fontsize =15)\n",
    "plt.ylabel('Number of Bad Words', fontsize=15);\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_gbw = data_gbw.mean(axis=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_gbw['mean'] = mean_gbw "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#status of every companies based on positive and negative wo\n",
    "data_gbw = data_gbw.sort_values(by ='mean', ascending = False)\n",
    "data_gbw.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_clean = pd.read_pickle('data_clean.pkl')\n",
    "data_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_companies = ['وبملت', 'خودرو', 'وتجارت', 'فولاد', 'وبصادر', 'ونوین', 'فملی', 'کچاد', 'وپارس', 'کگل', 'خساپا',\n",
    "                      'شیران', 'چکارن', 'فارس', 'وپاسار']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_companies = data_clean.loc[data_clean['hashtags'].isin(selected_companies)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#15 Top companies\n",
    "selected_companies.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = ['را', 'با', 'است', 'می', 'که', 'این', 'از', 'به', 'در', 'و', 'های', 'برای', 'آن', 'یک', 'ها',\n",
    "             'شود', 'شده', 'خود', 'کرد', 'ای', 'کرده', 'داشته', 'بوده', 'بود', 'زده', 'تا', 'هر', 'هم', 'نیز',\n",
    "             'خواهد', 'شد', 'بر', 'دارد', 'زد', 'آن', 'یا', 'باشد', 'میگردد', 'ای' ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "cv =  CountVectorizer(stop_words = stop_words)\n",
    "data_cv = cv.fit_transform(selected_companies.Post)\n",
    "data_dtm = pd.DataFrame(data_cv.toarray(), columns = cv.get_feature_names())\n",
    "data_dtm.index = selected_companies.hashtags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data_dtm.transpose()\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_dict = {}\n",
    "\n",
    "for c in data.columns:\n",
    "    top = data[c].sort_values(ascending = False).head(30)\n",
    "    top_dict[c] = list(zip(top.index, top.values))\n",
    "\n",
    "top_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at the most common top words --> add them to the stop word list\n",
    "from collections import Counter\n",
    "\n",
    "# Let's first pull out the top 30 words for each companies\n",
    "words = []\n",
    "for companies in data.columns:\n",
    "    top = [word for (word, count) in top_dict[companies]]\n",
    "    for t in top:\n",
    "        words.append(t)\n",
    "        \n",
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's aggregate this list and identify the most common words\n",
    "Counter(words).most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If more than half of the comedians have it as a top word, exclude it from the list\n",
    "add_stop_words = [word for word, count in Counter(words).most_common() if count > 6]\n",
    "add_stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = ['را', 'با', 'است', 'می', 'که', 'این', 'از', 'به', 'در', 'و', 'های', 'برای', 'آن', 'یک', 'ها',\n",
    "             'شود', 'شده', 'خود', 'کرد', 'ای', 'کرده', 'داشته', 'بوده', 'بود', 'زده', 'تا', 'هر', 'هم', 'نیز',\n",
    "             'خواهد', 'شد', 'بر', 'دارد', 'زد', 'آن', 'یا', 'باشد', 'میگردد', 'ای' ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's update our document-term matrix with the new list of stop words\n",
    "from sklearn.feature_extraction import text \n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "\n",
    "# Add new stop words\n",
    "stop_words = add_stop_words + stop_words\n",
    "\n",
    "# Recreate document-term matrix\n",
    "cv = CountVectorizer(stop_words=stop_words)\n",
    "data_cv = cv.fit_transform(selected_companies.Post)\n",
    "data_stop = pd.DataFrame(data_cv.toarray(), columns=cv.get_feature_names())\n",
    "data_stop.index = selected_companies.hashtags\n",
    "\n",
    "# Pickle it for later use\n",
    "import pickle\n",
    "pickle.dump(cv, open(\"cvs_stop.pkl\", \"wb\"))\n",
    "data_stop.to_pickle(\"dtms_stop.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the number of unique words that each companies uses\n",
    "\n",
    "unique_list = []\n",
    "for companies in data.columns:\n",
    "    uniques = data[companies].nonzero()[0].size\n",
    "    unique_list.append(uniques)\n",
    "data_words = pd.DataFrame(list(zip(selected_companies.hashtags, unique_list)), columns=['companies',\"unique_words\"])\n",
    "data_unique_sort = data_words.sort_values(by= \"unique_words\")\n",
    "data_unique_sort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find total number of words which a companies uses\n",
    "\n",
    "total_list = []\n",
    "\n",
    "for companies in data.columns:\n",
    "    totals = sum(data[companies])\n",
    "    total_list.append(totals)\n",
    "\n",
    "data_words['total_words'] = total_list\n",
    "\n",
    "data_wpm_sort = data_words.sort_values(by ='total_words' )\n",
    "data_wpm_sort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Counter(words).most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of negative words for every companies\n",
    "data_bad_words = data.transpose()[['زیان' ,'افت' ,'کاهش' ,'تعلیق' ,'منفی' ,'بدهی']]\n",
    "\n",
    "\n",
    "data_bw = pd.concat([data_bad_words.زیان + data_bad_words.افت +\n",
    "                     data_bad_words.کاهش + data_bad_words.تعلیق +\n",
    "                     data_bad_words.منفی + data_bad_words.بدهی]) \n",
    "\n",
    "data_bw = pd.DataFrame(data_bw, columns = ['bad_words'])\n",
    "data_bw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of positive words for every companies\n",
    "data_good_words = data.transpose()[['سود','افزایش','رشد','مثبت','پیشرفت','بهبود']]\n",
    "\n",
    "data_gw = pd.concat([data_good_words.پیشرفت + data_good_words.رشد +\n",
    "                     data_good_words.سود + data_good_words.افزایش +\n",
    "                     data_good_words.مثبت + data_good_words.بهبود])\n",
    "\n",
    "\n",
    "data_gw = pd.DataFrame(data_gw, columns = ['good_words'])\n",
    "data_gw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_gw['bad_words'] = data_bw.bad_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_gbw = data_gw.sort_values(by ='good_words', ascending = False)\n",
    "data_gbw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = [15, 10]\n",
    "\n",
    "for i, companies in enumerate(data_gbw.index):\n",
    "    x = data_gbw.good_words.loc[companies]\n",
    "    y = data_gbw.bad_words.loc[companies]\n",
    "    plt.scatter(x, y, color = \"blue\")\n",
    "    #plt.text(x+1.5, y+0.5, data_gbw.index[i], fontsize = 10 )\n",
    "    #plt.xlim(-5, 155)\n",
    "    \n",
    "plt.title('Number of Bad and Good Words')\n",
    "plt.xlabel('Number of Good Words', fontsize =15)\n",
    "plt.ylabel('Number of Bad Words', fontsize=15);\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's make some word clouds!\n",
    "# Terminal / Anaconda Prompt: conda install -c conda-forge wordcloud\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "wc = WordCloud(stopwords=stop_words, background_color=\"white\", colormap=\"Dark2\",\n",
    "               max_font_size=150, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset the output dimensions\n",
    "from __future__ import unicode_literals\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.rcParams['figure.figsize'] = [16, 6]\n",
    "\n",
    "company_names = ['وبملت', 'خودرو', 'وتجارت', 'فولاد', 'وبصادر', 'ونوین', 'فملی', 'کچاد', 'وپارس', 'کگل', 'خساپا',\n",
    "                 'شیران', 'چکارن', 'فارس', 'وپاسار']\n",
    "\n",
    "# Create subplots for each companies\n",
    "for index, companies in enumerate(data.columns):\n",
    "    wc.generate(str(selected_companies.Post))\n",
    "    \n",
    "    plt.subplot(3, 4, index+1)\n",
    "    plt.imshow(wc, interpolation=\"bilinear\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(company_names[index])\n",
    "    \n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
